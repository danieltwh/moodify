{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, ReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from ultralytics import YOLO\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.applications import VGG19 as vgg\n",
    "\n",
    "from matplotlib.pyplot import imshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is to train the final model for deployment into the app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#The data is read from the directory and each batch can be thought of as a set of labels and datapoints\n",
    "#Might need to change the directory depending on where\n",
    "\n",
    "#Seed 50 is decent performing\n",
    "train = keras.utils.image_dataset_from_directory(\n",
    "    directory='../../data/Faces updated',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=16,\n",
    "    image_size=(500, 500),\n",
    "   \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = train.map(lambda x, y: (keras.applications.vgg19.preprocess_input(x), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the output\n",
    "inputs = Input(shape=(500, 500, 3))\n",
    "\n",
    "#set inference mode to be false, flatten the output and add dense layers before making the final prediction\n",
    "vgg_output = keras.applications.VGG19(input_tensor=inputs,                                      \n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "\n",
    "    input_shape=(500, 500, 3),\n",
    "    pooling=None,\n",
    "\n",
    "\n",
    ")\n",
    "for layer in vgg_output.layers:\n",
    "    layer.trainable = False\n",
    "flatten = Flatten()(vgg_output.output)\n",
    "dense1 = Dense(256, activation = 'relu', name = 'first_dense_layer')(flatten)\n",
    "dropout1 = Dropout(0.25)(dense1)\n",
    "batch_norm1 = BatchNormalization(name = 'batch_norm1')(dropout1)\n",
    "dense2 = Dense(256, activation = 'relu', name = 'second_dense_layer')(batch_norm1)\n",
    "dropout2 = Dropout(0.25)(dense2)\n",
    "batch_norm1 = BatchNormalization(name = 'batch_norm2')(dropout2)\n",
    "output = Dense(3, activation = 'softmax', name = 'output')(batch_norm1)\n",
    "final_model = Model(inputs, output)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "final_model.compile(optimizer = 'adam', loss = CategoricalCrossentropy(), metrics = ['accuracy'])\n",
    "#save the model every epoch\n",
    "checkpoint = ModelCheckpoint(filepath = 'vggemotion.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 14s/step - accuracy: 0.3708 - loss: 1.6062\n",
      "Epoch 2/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 11s/step - accuracy: 0.6296 - loss: 0.9160\n",
      "Epoch 3/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 12s/step - accuracy: 0.6386 - loss: 0.7752\n",
      "Epoch 4/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 10s/step - accuracy: 0.7886 - loss: 0.5229\n",
      "Epoch 5/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8s/step - accuracy: 0.8166 - loss: 0.4942\n",
      "Epoch 6/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 12s/step - accuracy: 0.8720 - loss: 0.3679\n",
      "Epoch 7/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 13s/step - accuracy: 0.8798 - loss: 0.2855\n",
      "Epoch 8/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 20s/step - accuracy: 0.9080 - loss: 0.2719\n",
      "Epoch 9/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 20s/step - accuracy: 0.8892 - loss: 0.2595\n",
      "Epoch 10/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 18s/step - accuracy: 0.9455 - loss: 0.1674\n",
      "Epoch 11/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 19s/step - accuracy: 0.9552 - loss: 0.1538\n",
      "Epoch 12/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 19s/step - accuracy: 0.9369 - loss: 0.1477\n",
      "Epoch 13/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 18s/step - accuracy: 0.9546 - loss: 0.1610\n",
      "Epoch 14/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 14s/step - accuracy: 0.9505 - loss: 0.1698\n",
      "Epoch 15/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 11s/step - accuracy: 0.9525 - loss: 0.1409\n",
      "Epoch 16/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 14s/step - accuracy: 0.9755 - loss: 0.0829\n",
      "Epoch 17/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 15s/step - accuracy: 0.9607 - loss: 0.0829\n",
      "Epoch 18/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 20s/step - accuracy: 0.9600 - loss: 0.1046\n",
      "Epoch 19/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 19s/step - accuracy: 0.9798 - loss: 0.0560\n",
      "Epoch 20/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 15s/step - accuracy: 0.9704 - loss: 0.0730\n",
      "Epoch 21/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 17s/step - accuracy: 0.9727 - loss: 0.0778\n",
      "Epoch 22/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 18s/step - accuracy: 0.9972 - loss: 0.0274\n",
      "Epoch 23/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1714s\u001b[0m 169s/step - accuracy: 0.9823 - loss: 0.0429\n",
      "Epoch 24/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 14s/step - accuracy: 0.9663 - loss: 0.0872\n",
      "Epoch 25/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 16s/step - accuracy: 0.9783 - loss: 0.0499\n",
      "Epoch 26/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 15s/step - accuracy: 0.9937 - loss: 0.0309\n",
      "Epoch 27/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 16s/step - accuracy: 0.9969 - loss: 0.0188\n",
      "Epoch 28/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 9s/step - accuracy: 0.9872 - loss: 0.0444\n",
      "Epoch 29/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 14s/step - accuracy: 0.9956 - loss: 0.0403\n",
      "Epoch 30/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 15s/step - accuracy: 0.9831 - loss: 0.0693\n",
      "Epoch 31/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 15s/step - accuracy: 0.9965 - loss: 0.0260\n",
      "Epoch 32/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 16s/step - accuracy: 0.9689 - loss: 0.0854\n",
      "Epoch 33/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 15s/step - accuracy: 0.9867 - loss: 0.0437\n",
      "Epoch 34/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 17s/step - accuracy: 0.9772 - loss: 0.0634\n",
      "Epoch 35/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 13s/step - accuracy: 0.9816 - loss: 0.0718\n",
      "Epoch 36/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 9s/step - accuracy: 0.9990 - loss: 0.0281\n",
      "Epoch 37/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 11s/step - accuracy: 0.9950 - loss: 0.0469\n",
      "Epoch 38/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 12s/step - accuracy: 0.9727 - loss: 0.1058\n",
      "Epoch 39/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 12s/step - accuracy: 0.9974 - loss: 0.0350\n",
      "Epoch 40/40\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 12s/step - accuracy: 0.9918 - loss: 0.0665\n"
     ]
    }
   ],
   "source": [
    "final_model.fit(train_processed, epochs=40)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "final_model.save('vggemotion_final.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
