{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL \n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDetector:\n",
    "\n",
    "  def __init__(self, trained_model_path):\n",
    "    self.model = tf.keras.models.load_model(trained_model_path)\n",
    "\n",
    "\n",
    "  #Should be the path to a cropped face, majority of the image should be face if not not likely to work well,\n",
    "  #(can easily be changed to batch of images if necessary)\n",
    "  def get_emotion(self, path_to_cropped_face):\n",
    "    pil_image = PIL.Image.open(path_to_cropped_face).convert('L')\n",
    "\n",
    "    #reshape the pil image\n",
    "    pil_image = pil_image.resize((48,48))\n",
    "\n",
    "    #convert to numpy array\n",
    "    to_model = np.array(pil_image).reshape((1,48,48,1))\n",
    "\n",
    "    #return the softmax output, in the following order: 1. Angry, 2. Disgust, 3. Fear, 4. Happy, 5. Neutral, 6. Sad\n",
    "    #7.Surprise\n",
    "\n",
    "    #For now, extract the 1d array of emotion probabilities as per the order above, can be changed as necessary if\n",
    "    #input is batch\n",
    "    pred = self.model(to_model).numpy()[0]\n",
    "\n",
    "    return pred\n",
    "  \n",
    "  def analyse_emotion(self, path_to_user):\n",
    "    preds_str = []\n",
    "    interps = []\n",
    "\n",
    "    #can edit this as necessary, for example remove disgust and apply softmax as necessary, if delete emotions,\n",
    "    #adjust the index as necessary, alphabetical order is preserved due to how keras reads in directories\n",
    "    idx2emotion = {0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'neutral', 5:'sad', 6:'surprise'}\n",
    "    overall_emotion = {'angry':0, 'disgust':0, 'fear':0, 'happy':0, 'neutral':0, 'sad':0, 'surprise':0}\n",
    "    #get all the images in the path\n",
    "    for item in os.listdir(path_to_user):\n",
    "      cropped_img_path = os.path.join(path_to_user, item)\n",
    "      if os.path.isfile(cropped_img_path):\n",
    "        \n",
    "        pred = self.get_emotion(cropped_img_path)\n",
    "        #convert the predictions to indexes\n",
    "        idx = np.argmax(pred)\n",
    "        #delete emotions as necessary\n",
    "\n",
    "        #run softmax function IF DELETE INDEX, ELSE NN output is already normalized\n",
    "        #pred = np.exp(pred)/np.sum(pred)\n",
    "\n",
    "        emotion = idx2emotion[idx]\n",
    "        preds_str.append(emotion)\n",
    "\n",
    "        #create the dictionary for the interps\n",
    "        current_iteration = {}\n",
    "\n",
    "        for i,score in enumerate(list(pred)):\n",
    "          emotion = idx2emotion[i]\n",
    "          current_iteration[emotion] = score\n",
    "          overall_emotion[emotion] += score\n",
    "        \n",
    "        interps.append(current_iteration)\n",
    "      \n",
    "    \n",
    "    overall_face_emotion = sorted(overall_emotion.items(), key=lambda x:x[1], reverse = True)[0][0]\n",
    "\n",
    "    return overall_face_emotion, preds_str, interps \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionDetector(r'src/Mood_detector/trained_emotion.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000168179213"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.get_emotion('uploads/test/img/crops/face/video_1.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploads/test/img/crops/facevideo_1.jpg\n",
      "uploads/test/img/crops/facevideo_10.jpg\n",
      "uploads/test/img/crops/facevideo_2.jpg\n",
      "uploads/test/img/crops/facevideo_3.jpg\n",
      "uploads/test/img/crops/facevideo_4.jpg\n",
      "uploads/test/img/crops/facevideo_5.jpg\n",
      "uploads/test/img/crops/facevideo_6.jpg\n",
      "uploads/test/img/crops/facevideo_7.jpg\n",
      "uploads/test/img/crops/facevideo_8.jpg\n",
      "uploads/test/img/crops/facevideo_9.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('neutral',\n",
       " ['neutral',\n",
       "  'neutral',\n",
       "  'sad',\n",
       "  'happy',\n",
       "  'happy',\n",
       "  'neutral',\n",
       "  'fear',\n",
       "  'neutral',\n",
       "  'sad',\n",
       "  'sad'],\n",
       " [{'angry': 0.026952071,\n",
       "   'disgust': 3.3114443e-06,\n",
       "   'fear': 0.04647292,\n",
       "   'happy': 0.0010000556,\n",
       "   'neutral': 0.74368906,\n",
       "   'sad': 0.18170615,\n",
       "   'surprise': 0.00017645482},\n",
       "  {'angry': 0.00598501,\n",
       "   'disgust': 2.6504136e-08,\n",
       "   'fear': 0.019079035,\n",
       "   'happy': 7.7492135e-05,\n",
       "   'neutral': 0.8727128,\n",
       "   'sad': 0.102134235,\n",
       "   'surprise': 1.1328599e-05},\n",
       "  {'angry': 0.03692203,\n",
       "   'disgust': 0.00027440983,\n",
       "   'fear': 0.09771531,\n",
       "   'happy': 0.0028511651,\n",
       "   'neutral': 0.1370125,\n",
       "   'sad': 0.72422576,\n",
       "   'surprise': 0.0009988273},\n",
       "  {'angry': 9.94471e-09,\n",
       "   'disgust': 2.7109953e-17,\n",
       "   'fear': 8.409566e-08,\n",
       "   'happy': 0.99999845,\n",
       "   'neutral': 1.4341771e-06,\n",
       "   'sad': 3.3089805e-08,\n",
       "   'surprise': 3.176606e-08},\n",
       "  {'angry': 0.0022087698,\n",
       "   'disgust': 7.3140285e-07,\n",
       "   'fear': 0.005330407,\n",
       "   'happy': 0.9560441,\n",
       "   'neutral': 0.020052463,\n",
       "   'sad': 0.01568932,\n",
       "   'surprise': 0.0006742159},\n",
       "  {'angry': 0.0061385008,\n",
       "   'disgust': 3.3224762e-07,\n",
       "   'fear': 0.008233912,\n",
       "   'happy': 0.2108598,\n",
       "   'neutral': 0.7287687,\n",
       "   'sad': 0.045875676,\n",
       "   'surprise': 0.00012311812},\n",
       "  {'angry': 0.20901299,\n",
       "   'disgust': 0.0017061175,\n",
       "   'fear': 0.36079526,\n",
       "   'happy': 0.014212934,\n",
       "   'neutral': 0.10232713,\n",
       "   'sad': 0.30031517,\n",
       "   'surprise': 0.011630353},\n",
       "  {'angry': 0.00094246183,\n",
       "   'disgust': 7.0814006e-12,\n",
       "   'fear': 0.002330614,\n",
       "   'happy': 1.944841e-06,\n",
       "   'neutral': 0.98818016,\n",
       "   'sad': 0.008544441,\n",
       "   'surprise': 3.248953e-07},\n",
       "  {'angry': 0.020759653,\n",
       "   'disgust': 3.5505684e-05,\n",
       "   'fear': 0.085727975,\n",
       "   'happy': 0.0012107104,\n",
       "   'neutral': 0.23882824,\n",
       "   'sad': 0.6528678,\n",
       "   'surprise': 0.0005702059},\n",
       "  {'angry': 0.035190996,\n",
       "   'disgust': 6.836303e-05,\n",
       "   'fear': 0.10219689,\n",
       "   'happy': 0.0025475381,\n",
       "   'neutral': 0.40177673,\n",
       "   'sad': 0.4570546,\n",
       "   'surprise': 0.0011648075}])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.analyse_emotion('uploads/test/img/crops/face')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
